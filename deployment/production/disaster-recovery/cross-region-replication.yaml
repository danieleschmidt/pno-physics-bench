# Cross-Region Replication Configuration for PNO Physics Bench
# Multi-region deployment with automatic failover and data synchronization

---
# Global Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: global-replication-config
  namespace: production
data:
  replication.yaml: |
    global:
      primary_region: us-east-1
      backup_regions:
        - us-west-2
        - eu-west-1
        - ap-northeast-1
      
      rto_minutes: 15  # Recovery Time Objective
      rpo_minutes: 60  # Recovery Point Objective
      
      failover:
        automatic: true
        health_check_interval: 30
        failure_threshold: 3
        
      data_sync:
        interval_minutes: 15
        compression: true
        encryption: true

---
# Cross-Region Storage Replication
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pno-model-storage-replica
  namespace: production
  annotations:
    volume.beta.kubernetes.io/storage-class: "fast-ssd"
    replication.io/regions: "us-west-2,eu-west-1"
    replication.io/sync-mode: "async"
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 500Gi
  storageClassName: fast-ssd

---
# Backup Storage Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-storage-config
  namespace: production
data:
  backup-regions.json: |
    {
      "us-east-1": {
        "bucket": "pno-backups-us-east-1",
        "storage_class": "STANDARD_IA",
        "encryption": "AES256",
        "lifecycle_policy": {
          "transition_to_glacier": 30,
          "delete_after": 90
        }
      },
      "us-west-2": {
        "bucket": "pno-backups-us-west-2", 
        "storage_class": "STANDARD_IA",
        "encryption": "AES256",
        "lifecycle_policy": {
          "transition_to_glacier": 30,
          "delete_after": 90
        }
      },
      "eu-west-1": {
        "bucket": "pno-backups-eu-west-1",
        "storage_class": "STANDARD_IA", 
        "encryption": "AES256",
        "lifecycle_policy": {
          "transition_to_glacier": 30,
          "delete_after": 90
        }
      },
      "ap-northeast-1": {
        "bucket": "pno-backups-ap-northeast-1",
        "storage_class": "STANDARD_IA",
        "encryption": "AES256", 
        "lifecycle_policy": {
          "transition_to_glacier": 30,
          "delete_after": 90
        }
      }
    }

---
# Cross-Region Sync Job
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cross-region-sync
  namespace: production
spec:
  schedule: "*/15 * * * *"  # Every 15 minutes
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: sync-job
            image: amazon/aws-cli:latest
            command:
            - /bin/bash
            - -c
            - |
              set -e
              
              # Configuration
              PRIMARY_REGION="us-east-1"
              BACKUP_REGIONS=("us-west-2" "eu-west-1" "ap-northeast-1")
              PRIMARY_BUCKET="pno-backups-us-east-1"
              
              echo "Starting cross-region sync at $(date)"
              
              # Sync to each backup region
              for region in "${BACKUP_REGIONS[@]}"; do
                BACKUP_BUCKET="pno-backups-${region}"
                echo "Syncing to region: $region (bucket: $BACKUP_BUCKET)"
                
                # Sync backups
                aws s3 sync s3://${PRIMARY_BUCKET}/backups/ s3://${BACKUP_BUCKET}/backups/ \
                  --source-region ${PRIMARY_REGION} \
                  --region ${region} \
                  --exclude "*" \
                  --include "*.tar.gz" \
                  --include "*.json" \
                  --storage-class STANDARD_IA
                
                # Sync metadata
                aws s3 sync s3://${PRIMARY_BUCKET}/metadata/ s3://${BACKUP_BUCKET}/metadata/ \
                  --source-region ${PRIMARY_REGION} \
                  --region ${region}
                  
                echo "Completed sync to $region"
              done
              
              echo "Cross-region sync completed at $(date)"
            env:
            - name: AWS_DEFAULT_REGION
              value: us-east-1
            volumeMounts:
            - name: aws-credentials
              mountPath: /root/.aws
              readOnly: true
          volumes:
          - name: aws-credentials
            secret:
              secretName: aws-credentials

---
# Regional Health Check Service
apiVersion: v1
kind: Service
metadata:
  name: regional-health-check
  namespace: production
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
spec:
  type: LoadBalancer
  selector:
    app: pno-physics-bench
  ports:
  - port: 80
    targetPort: 8000
    name: http
  - port: 443
    targetPort: 8000
    name: https

---
# Failover Controller Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: failover-controller
  namespace: production
spec:
  replicas: 1
  selector:
    matchLabels:
      app: failover-controller
  template:
    metadata:
      labels:
        app: failover-controller
    spec:
      serviceAccountName: failover-controller
      containers:
      - name: controller
        image: pno-failover-controller:v1.0.0
        env:
        - name: PRIMARY_REGION
          value: "us-east-1"
        - name: BACKUP_REGIONS
          value: "us-west-2,eu-west-1,ap-northeast-1"
        - name: CHECK_INTERVAL
          value: "30"
        - name: FAILURE_THRESHOLD
          value: "3"
        - name: DNS_ZONE_ID
          value: "Z123456789ABCDEFGHIJ"
        - name: HEALTH_CHECK_URL
          value: "https://api.pno-physics-bench.com/health"
        command:
        - /bin/bash
        - -c
        - |
          #!/bin/bash
          set -e
          
          PRIMARY_REGION=${PRIMARY_REGION:-us-east-1}
          BACKUP_REGIONS=${BACKUP_REGIONS:-us-west-2,eu-west-1}
          CHECK_INTERVAL=${CHECK_INTERVAL:-30}
          FAILURE_THRESHOLD=${FAILURE_THRESHOLD:-3}
          HEALTH_CHECK_URL=${HEALTH_CHECK_URL:-https://api.pno-physics-bench.com/health}
          
          failure_count=0
          current_active_region=$PRIMARY_REGION
          
          log() {
            echo "[$(date)] $1"
          }
          
          check_region_health() {
            local region=$1
            local health_url="https://api-${region}.pno-physics-bench.com/health"
            
            if curl -f -s --max-time 10 "$health_url" > /dev/null 2>&1; then
              return 0
            else
              return 1
            fi
          }
          
          failover_to_region() {
            local target_region=$1
            log "Initiating failover to region: $target_region"
            
            # Update DNS to point to backup region
            aws route53 change-resource-record-sets \
              --hosted-zone-id $DNS_ZONE_ID \
              --change-batch '{
                "Changes": [{
                  "Action": "UPSERT",
                  "ResourceRecordSet": {
                    "Name": "api.pno-physics-bench.com",
                    "Type": "CNAME",
                    "TTL": 60,
                    "ResourceRecords": [{"Value": "api-'$target_region'.pno-physics-bench.com"}]
                  }
                }]
              }'
            
            # Notify operations team
            curl -X POST -H 'Content-type: application/json' \
              --data '{"text":"🚨 FAILOVER: Switched traffic from '$current_active_region' to '$target_region'"}' \
              $SLACK_WEBHOOK_URL
            
            current_active_region=$target_region
            log "Failover completed to $target_region"
          }
          
          log "Starting failover controller"
          log "Primary region: $PRIMARY_REGION"
          log "Backup regions: $BACKUP_REGIONS"
          log "Check interval: ${CHECK_INTERVAL}s"
          log "Failure threshold: $FAILURE_THRESHOLD"
          
          while true; do
            # Check current active region health
            if check_region_health $current_active_region; then
              failure_count=0
              log "Region $current_active_region is healthy"
            else
              failure_count=$((failure_count + 1))
              log "Health check failed for $current_active_region (failure count: $failure_count)"
              
              # If failure threshold reached, initiate failover
              if [ $failure_count -ge $FAILURE_THRESHOLD ]; then
                log "Failure threshold reached, initiating failover"
                
                # Find healthy backup region
                IFS=',' read -ra REGIONS <<< "$BACKUP_REGIONS"
                for backup_region in "${REGIONS[@]}"; do
                  if [ "$backup_region" != "$current_active_region" ]; then
                    if check_region_health $backup_region; then
                      failover_to_region $backup_region
                      failure_count=0
                      break
                    else
                      log "Backup region $backup_region is also unhealthy"
                    fi
                  fi
                done
              fi
            fi
            
            sleep $CHECK_INTERVAL
          done
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
        volumeMounts:
        - name: aws-credentials
          mountPath: /root/.aws
          readOnly: true
      volumes:
      - name: aws-credentials
        secret:
          secretName: aws-credentials

---
# Failover Controller ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: failover-controller
  namespace: production
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/PNOFailoverRole

---
# Failover Controller RBAC
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: production
  name: failover-controller
rules:
- apiGroups: [""]
  resources: ["pods", "services", "endpoints"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "patch"]
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses"]
  verbs: ["get", "list", "watch", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: failover-controller
  namespace: production
subjects:
- kind: ServiceAccount
  name: failover-controller
  namespace: production
roleRef:
  kind: Role
  name: failover-controller
  apiGroup: rbac.authorization.k8s.io

---
# Data Synchronization Job
apiVersion: batch/v1
kind: CronJob
metadata:
  name: data-sync-job
  namespace: production
spec:
  schedule: "0 */4 * * *"  # Every 4 hours
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: data-sync
            image: pno-physics-bench:v1.0.0
            command:
            - /bin/bash
            - -c
            - |
              set -e
              
              log() {
                echo "[$(date)] $1"
              }
              
              log "Starting data synchronization"
              
              # Sync model files across regions
              PRIMARY_MODELS_PATH="/app/models"
              BACKUP_REGIONS=("us-west-2" "eu-west-1" "ap-northeast-1")
              
              # Create model checkpoint
              log "Creating model checkpoint"
              tar -czf /tmp/models-$(date +%Y%m%d-%H%M%S).tar.gz -C $PRIMARY_MODELS_PATH .
              
              # Upload to S3 for replication
              aws s3 cp /tmp/models-*.tar.gz s3://pno-model-sync/latest/models.tar.gz
              
              # Trigger replication to backup regions
              for region in "${BACKUP_REGIONS[@]}"; do
                log "Triggering sync to $region"
                aws s3 sync s3://pno-model-sync/latest/ s3://pno-model-sync-${region}/latest/ \
                  --region $region
              done
              
              # Verify model consistency
              CHECKSUM=$(sha256sum /tmp/models-*.tar.gz | awk '{print $1}')
              echo $CHECKSUM > /tmp/model-checksum.txt
              
              aws s3 cp /tmp/model-checksum.txt s3://pno-model-sync/latest/checksum.txt
              
              log "Data synchronization completed"
              
              # Cleanup
              rm -f /tmp/models-*.tar.gz /tmp/model-checksum.txt
            env:
            - name: AWS_DEFAULT_REGION
              value: us-east-1
            volumeMounts:
            - name: model-storage
              mountPath: /app/models
              readOnly: true
            - name: aws-credentials
              mountPath: /root/.aws
              readOnly: true
          volumes:
          - name: model-storage
            persistentVolumeClaim:
              claimName: pno-model-storage
          - name: aws-credentials
            secret:
              secretName: aws-credentials

---
# Regional Deployment Status Monitor
apiVersion: v1
kind: ConfigMap
metadata:
  name: regional-status-monitor
  namespace: production
data:
  monitor.py: |
    #!/usr/bin/env python3
    import asyncio
    import aiohttp
    import json
    import logging
    import os
    from datetime import datetime
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    class RegionalStatusMonitor:
        def __init__(self):
            self.regions = {
                'us-east-1': 'https://api-us-east-1.pno-physics-bench.com',
                'us-west-2': 'https://api-us-west-2.pno-physics-bench.com', 
                'eu-west-1': 'https://api-eu-west-1.pno-physics-bench.com',
                'ap-northeast-1': 'https://api-ap-northeast-1.pno-physics-bench.com'
            }
            self.status_file = '/tmp/regional-status.json'
            
        async def check_all_regions(self):
            """Check health status of all regions"""
            
            status_results = {
                'timestamp': datetime.now().isoformat(),
                'regions': {}
            }
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                tasks = []
                
                for region, url in self.regions.items():
                    task = self.check_region_health(session, region, url)
                    tasks.append(task)
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for region, result in zip(self.regions.keys(), results):
                    if isinstance(result, Exception):
                        status_results['regions'][region] = {
                            'status': 'error',
                            'error': str(result),
                            'response_time': None
                        }
                    else:
                        status_results['regions'][region] = result
            
            # Save status to file
            with open(self.status_file, 'w') as f:
                json.dump(status_results, f, indent=2)
            
            return status_results
            
        async def check_region_health(self, session, region, base_url):
            """Check health of a specific region"""
            
            start_time = asyncio.get_event_loop().time()
            
            try:
                health_url = f"{base_url}/health"
                
                async with session.get(health_url) as response:
                    response_time = (asyncio.get_event_loop().time() - start_time) * 1000
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        return {
                            'status': 'healthy',
                            'response_time': response_time,
                            'details': data
                        }
                    else:
                        return {
                            'status': 'unhealthy',
                            'response_time': response_time,
                            'http_status': response.status
                        }
                        
            except Exception as e:
                return {
                    'status': 'error',
                    'error': str(e),
                    'response_time': None
                }
    
    async def main():
        monitor = RegionalStatusMonitor()
        
        while True:
            try:
                logger.info("Checking regional status...")
                status = await monitor.check_all_regions()
                
                # Log summary
                healthy_regions = [r for r, s in status['regions'].items() if s['status'] == 'healthy']
                logger.info(f"Healthy regions: {len(healthy_regions)}/{len(status['regions'])}")
                
                for region, region_status in status['regions'].items():
                    if region_status['status'] != 'healthy':
                        logger.warning(f"Region {region}: {region_status['status']}")
                
                # Wait 1 minute before next check
                await asyncio.sleep(60)
                
            except Exception as e:
                logger.error(f"Regional status check failed: {e}")
                await asyncio.sleep(60)
    
    if __name__ == "__main__":
        asyncio.run(main())

---
# Regional Status Monitor Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: regional-status-monitor
  namespace: production
spec:
  replicas: 1
  selector:
    matchLabels:
      app: regional-status-monitor
  template:
    metadata:
      labels:
        app: regional-status-monitor
    spec:
      containers:
      - name: monitor
        image: python:3.9-slim
        command:
        - /bin/bash
        - -c
        - |
          pip install aiohttp
          python /app/monitor.py
        volumeMounts:
        - name: monitor-script
          mountPath: /app
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 100m
            memory: 128Mi
      volumes:
      - name: monitor-script
        configMap:
          name: regional-status-monitor
          defaultMode: 0755

---
# Disaster Recovery Testing Job
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-test-job
  namespace: production
spec:
  schedule: "0 3 * * 0"  # Weekly on Sunday at 3 AM
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: dr-test
            image: pno-physics-bench:v1.0.0
            command:
            - /bin/bash
            - -c
            - |
              set -e
              
              log() {
                echo "[$(date)] $1"
              }
              
              log "Starting disaster recovery test"
              
              # Test backup integrity
              log "Testing backup integrity..."
              python3 /opt/pno/disaster-recovery/disaster-recovery-automation.py \
                --action cleanup
              
              # Test cross-region replication
              log "Testing cross-region replication..."
              
              REGIONS=("us-west-2" "eu-west-1" "ap-northeast-1")
              PRIMARY_BUCKET="pno-backups-us-east-1"
              
              # Get latest backup from primary
              LATEST_BACKUP=$(aws s3 ls s3://$PRIMARY_BUCKET/backups/ --recursive | \
                sort | tail -1 | awk '{print $4}')
              
              if [ -z "$LATEST_BACKUP" ]; then
                log "ERROR: No backups found in primary region"
                exit 1
              fi
              
              log "Testing replication of: $LATEST_BACKUP"
              
              # Verify backup exists in all regions
              for region in "${REGIONS[@]}"; do
                BACKUP_BUCKET="pno-backups-${region}"
                
                if aws s3 ls s3://$BACKUP_BUCKET/$LATEST_BACKUP --region $region > /dev/null 2>&1; then
                  log "✅ Backup found in $region"
                else
                  log "❌ Backup NOT found in $region"
                  exit 1
                fi
              done
              
              # Test failover simulation (without actually switching traffic)
              log "Testing failover simulation..."
              
              for region in "${REGIONS[@]}"; do
                HEALTH_URL="https://api-${region}.pno-physics-bench.com/health"
                
                if curl -f -s --max-time 10 "$HEALTH_URL" > /dev/null; then
                  log "✅ Region $region is ready for failover"
                else
                  log "❌ Region $region is NOT ready for failover"
                fi
              done
              
              # Generate DR test report
              cat > /tmp/dr-test-report.json << EOF
              {
                "test_date": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
                "backup_integrity": "PASS",
                "cross_region_replication": "PASS", 
                "failover_readiness": "PASS",
                "recommendations": [
                  "All disaster recovery components are functioning correctly",
                  "Cross-region replication is working properly",
                  "Backup regions are ready for failover"
                ]
              }
              EOF
              
              # Upload report
              aws s3 cp /tmp/dr-test-report.json \
                s3://pno-backups-us-east-1/reports/dr-test-$(date +%Y%m%d).json
              
              log "Disaster recovery test completed successfully"
              
              # Send success notification
              curl -X POST -H 'Content-type: application/json' \
                --data '{"text":"✅ Weekly DR test completed successfully"}' \
                $SLACK_WEBHOOK_URL
            env:
            - name: AWS_DEFAULT_REGION
              value: us-east-1
            - name: SLACK_WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: notifications
                  key: slack-webhook-url
            volumeMounts:
            - name: aws-credentials
              mountPath: /root/.aws
              readOnly: true
          volumes:
          - name: aws-credentials
            secret:
              secretName: aws-credentials